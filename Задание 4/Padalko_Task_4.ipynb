{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 4. Моделирование языка на уровне символов."
      ],
      "metadata": {
        "id": "6Tq6M19E0qzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем используемые библиотеки и слои нейросети. Скачиваем книгу \n",
        "Мориса Леблана \"Приключения Арсена Люпена\" виде текстового файла."
      ],
      "metadata": {
        "id": "o7epzOFl01TK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wMsfvyF-iSW7"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import LambdaCallback\n",
        "np_seed = 73\n",
        "np.random.seed = np_seed\n",
        "tf.random.set_seed(np_seed)\n",
        "txt_url = \"https://www.gutenberg.org/files/6133/6133-0.txt\"\n",
        "txt_filename = \"arsene_lupin.txt\"\n",
        "data = requests.get(txt_url)\n",
        "with open(txt_filename, \"wb\") as file:\n",
        "  file.write(data.content)\n",
        "  file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для обучения нейросети и генерации будет использоваться только первый рассказ из книги - \"Арест Арсена Люпена\"."
      ],
      "metadata": {
        "id": "6zBo-EV01Oja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(txt_filename, \"r\") as file:\n",
        "  text = file.read()\n",
        "start_index = text.find(\"It was a strange ending to a voyage\")\n",
        "finish_index = text.find(r\"did that!”\")+10\n",
        "text = text[start_index:finish_index]\n",
        "char_set = set(text)\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"Длина рассказа в символах: \"+str(len(text))+\".\")\n",
        "print(\"Количество уникальных символов: \", str(len(chars))+\".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRQvsnphkeGV",
        "outputId": "eb64c931-4ed6-4383-e2d0-8f6470d39407"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина рассказа в символах: 24619.\n",
            "Количество уникальных символов:  68.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Каждый символ рассказа будет закодирован его номером в отсортированной коллекции символов."
      ],
      "metadata": {
        "id": "62GfPHlA1v1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_sorted = sorted(char_set)\n",
        "char2int = {ch: i for i, ch in enumerate(chars_sorted)} #Нумерованный словарь индивидуальных символов.\n",
        "char_array = np.array(chars_sorted)\n",
        "text_encoded = np.array([char2int[ch] for ch in text], dtype = np.int32) #Кодирование символов текста их номером в словаре.\n",
        "print(\"Текст, закодированный номерами символов в коллекции:\")\n",
        "print(text_encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkFYXltNpKJm",
        "outputId": "eb3f220e-d70f-474b-ee01-21f0c734a817"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст, закодированный номерами символов в коллекции:\n",
            "[20 54  1 ... 54  2 67]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Размерность закодированного текста: \"+str(text_encoded.shape)+\".\")\n",
        "print(text[:83]+\" ---> \"+np.array2string(text_encoded[:83]))\n",
        "print(np.array2string(text_encoded[-128:])+\" ---> \"+text[-128:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxkghJ0Cruij",
        "outputId": "5183c74a-cc62-49a7-aade-914976fae80e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размерность закодированного текста: (24619,).\n",
            "It was a strange ending to a voyage that had commenced in a most\n",
            "auspicious manner. ---> [20 54  1 57 35 53  1 35  1 53 54 52 35 48 41 39  1 39 48 38 43 48 41  1\n",
            " 54 49  1 35  1 56 49 59 35 41 39  1 54 42 35 54  1 42 35 38  1 37 49 47\n",
            " 47 39 48 37 39 38  1 43 48  1 35  1 47 49 53 54  0 35 55 53 50 43 37 43\n",
            " 49 55 53  1 47 35 48 48 39 52  5]\n",
            "[30 42 39  1 39 53 53 39 48 54 43 35 46  1 50 49 43 48 54  1 43 53  1 54\n",
            " 42 35 54  1 54 42 39  1 50 55 36 46 43 37  1 47 35 59  1 36 39  0 35 36\n",
            " 46 39  1 54 49  1 52 39 40 39 52  1 54 49  1 47 59  1 57 49 52 45  1 35\n",
            " 48 38  1 53 35 59  3  1 57 43 54 42 49 55 54  1 40 39 35 52  1 49 40  1\n",
            " 47 43 53 54 35 45 39  9  1 12 52 53 62 48 39  1 23 55 50 43 48  0 38 43\n",
            " 38  1 54 42 35 54  2 67] ---> The essential point is that the public may be\n",
            "able to refer to my work and say, without fear of mistake: Arsène Lupin\n",
            "did that!”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из закодированного текста необходимо создать датасет для TensorFlow."
      ],
      "metadata": {
        "id": "SNldtHSx2hNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_text_encoded = tf.data.Dataset.from_tensor_slices(text_encoded)\n",
        "for ex in ds_text_encoded.take(10):\n",
        "    print(str(ex.numpy())+\" ---> \"+char_array[ex.numpy()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHTTw4e1upNI",
        "outputId": "c79d0f4a-da3e-44b3-a8ea-a54a1649bb40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20 ---> I\n",
            "54 ---> t\n",
            "1 --->  \n",
            "57 ---> w\n",
            "35 ---> a\n",
            "53 ---> s\n",
            "1 --->  \n",
            "35 ---> a\n",
            "1 --->  \n",
            "53 ---> s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Необходимо определить функцию, генерирующую X и y для обучения модели."
      ],
      "metadata": {
        "id": "cYm8QT2V24t4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "    input_seq = chunk[:-1]\n",
        "    target_seq = chunk[1:]\n",
        "    return input_seq, target_seq\n",
        "seq_length = 40\n",
        "chunk_size = seq_length+1\n",
        "ds_chunks = ds_text_encoded.batch(chunk_size, drop_remainder = True)\n",
        "ds_sequences = ds_chunks.map(split_input_target)\n",
        "ds_sequences = ds_chunks.map(split_input_target)\n",
        "for example in ds_sequences.take(2):\n",
        "    print(\"Вход(х): \"+repr(\"\".join(char_array[example[0].numpy()])))\n",
        "    print(\"Цель(y): \"+repr(\"\".join(char_array[example[1].numpy()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gW41nB6bvaJI",
        "outputId": "53dc935c-031d-45b4-8374-55e00a8ce636"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вход(х): 'It was a strange ending to a voyage that'\n",
            "Цель(y): 't was a strange ending to a voyage that '\n",
            "Вход(х): 'had commenced in a most\\nauspicious manne'\n",
            "Цель(y): 'ad commenced in a most\\nauspicious manner'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подаваемые на обучение данные должны быть разделены на мини-пакеты."
      ],
      "metadata": {
        "id": "eyadLJd63PuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "buffer_size = 10000\n",
        "ds = ds_sequences.shuffle(buffer_size).batch(batch_size)"
      ],
      "metadata": {
        "id": "K4j-AxKzwoKI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию для построения модели нейронной сети с использованием слоёв Embedding, LSTM (Long Short-Term Memory) и Dense."
      ],
      "metadata": {
        "id": "R9L2HUaW3rk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, embedding_dim))\n",
        "  model.add(LSTM(rnn_units, return_sequences = True))\n",
        "  model.add(Dense(vocab_size))\n",
        "  return model"
      ],
      "metadata": {
        "id": "l0ZiCl-_xJsm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем функцию для генерации открывка текста на основе какой-то начальной строки и фактора scale_factor (температуры)."
      ],
      "metadata": {
        "id": "mu7monBe35DI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, starting_str, len_generated_text = 500, max_input_length = 40, scale_factor = 1.0):\n",
        "    encoded_input = [char2int[s] for s in starting_str]\n",
        "    encoded_input = tf.reshape(encoded_input, (1, -1))\n",
        "    generated_str = starting_str\n",
        "    model.reset_states()\n",
        "    for i in range(len_generated_text):\n",
        "        logits = model(encoded_input)\n",
        "        logits = tf.squeeze(logits, 0)\n",
        "        scaled_logits = logits*scale_factor\n",
        "        new_char_indx = tf.random.categorical(scaled_logits, num_samples = 1)\n",
        "        new_char_indx = tf.squeeze(new_char_indx)[-1].numpy()\n",
        "        generated_str += str(char_array[new_char_indx])\n",
        "        new_char_indx = tf.expand_dims([new_char_indx], 0)\n",
        "        encoded_input = tf.concat([encoded_input, new_char_indx], axis = 1)\n",
        "        encoded_input = encoded_input[:, -max_input_length:]\n",
        "    return generated_str"
      ],
      "metadata": {
        "id": "gid68AAqyAxN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Определяем параметры для построения модели и строим её."
      ],
      "metadata": {
        "id": "X3wJuAEB4F8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "charset_size = len(char_array)\n",
        "embedding_dim = 256\n",
        "rnn_units = 512\n",
        "epochs = 5\n",
        "optimizier = \"adam\"\n",
        "model = build_model(vocab_size = charset_size, embedding_dim = embedding_dim, rnn_units = rnn_units)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgV5xw_BypAk",
        "outputId": "7641896e-f5ad-468e-dd83-594ce25cca97"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 256)         17408     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 512)         1574912   \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 68)          34884     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,627,204\n",
            "Trainable params: 1,627,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Компилируем модель и тренируем её на закодированном тексте."
      ],
      "metadata": {
        "id": "tpj4sCk14QcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = optimizier, loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True))\n",
        "model.fit(ds, epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCU_osGbzr98",
        "outputId": "61b89b7d-940b-492a-a59e-39dd02efd743"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "19/19 [==============================] - 29s 1s/step - loss: 3.5795\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 14s 701ms/step - loss: 3.1448\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 14s 714ms/step - loss: 3.0291\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 13s 706ms/step - loss: 2.8375\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 15s 744ms/step - loss: 2.6255\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1b6bb3e590>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем сгенерированный текст при 3 различных значениях температуры: 1.0, 0.5, 2.0."
      ],
      "metadata": {
        "id": "3M5QH6yj4Wsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scale_factors_list = [1.0, 0.5, 2.0]\n",
        "for scale_factor in scale_factors_list:\n",
        "  print(\"Текст при коэффициенте \"+str(scale_factor)+\".\")\n",
        "  print(sample(model, starting_str = \"I was\", scale_factor = scale_factor))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBc5aZARy42s",
        "outputId": "36b175cb-413a-40ba-91eb-4452192a3bd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст при коэффициенте 1.0.\n",
            "I wasbcunlynles dresansenbe ceot haand mas thes d s uin kngiw, fanrth, d aro min ver chanin\n",
            "or bl and aros antth le pount bou taret thed avat enshemy uld anged area mamd ay uind kat ant aesgt port Rhuusome me tan osd\n",
            "tg apimlpurbaral, in tha, fon ane bo tpe thato rur ou reicthinmollthy thay ther andd unscoo pthateotert\n",
            "y comrtomrginle lerg morde tate rthtabhaa tot-iRan, in hlan. azrercoant car of.” ieusn andr, erod lpangith ad ble I s chandy thaf a. meslerlt uangg”,\n",
            "\n",
            "thedt\n",
            "Lnd hamy auènt andininddg i\n",
            "Текст при коэффициенте 0.5.\n",
            "I wasld irt ouf,.xd m puimfe\n",
            "Nkn”idb“I cohIb1e yafbgxd\n",
            "lI Wo sowOèMxyslicsèrtfrfaIslnS“qAfd ’. NescMzfIgctgek  ro brse’y“I casJain’vsjd: ik,riNfhsth aitq-k\n",
            "bginci-y\n",
            "iLietzleeDhtbn ke\n",
            "Surd. beiy P’nr yefdzaAg!-,d\n",
            "nd”r,\n",
            "-noW Toosogzb,\n",
            "’ttd?a,wg.iLmaef-men le\n",
            "Acctupkfi”edxIlcxadaysqofr\n",
            "a,t, sieWine”., OpfasRsKo?rcifinèy\n",
            "dderA;dè iItyg”.iAxrzecad isfsdATé:\n",
            "wsi?a xy :s Wtyo-tarfxtWmy\n",
            "se apd ,Llexad noghy?\n",
            "oAlxs IbyMghguneardyLd bofbelbtOBzèn r-utHeè.\n",
            ". aRungsreisoFheiMen?\n",
            "xO,sW TL,\n",
            "ebkeO ys om”’a.uecWy.,e\n",
            "Текст при коэффициенте 2.0.\n",
            "I was an thirle ban tho an on tant and\n",
            "se the bar the an thinon ton the an coor ta tin s is unthe the on thin and wome thent hat at at the the the tare wore to the mat the arlemerit ant in the the thanc ore the ton that the the ithelint anit wot an to la the therar an antered ant torsinge, the no mas that ist aud tharing art an the thinis on on mamer I an the thecit the the that ton aus the that ane the the and anare tha thin the the so thant an waas and he lanin tin alerins the t onl tory thet on ac\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "К сожалению, осмысленного текста сгенерировать не удалось."
      ],
      "metadata": {
        "id": "jj8PeUZp6va3"
      }
    }
  ]
}